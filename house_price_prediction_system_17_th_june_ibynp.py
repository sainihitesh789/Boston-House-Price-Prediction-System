# -*- coding: utf-8 -*-
"""HOUSE PRICE PREDICTION SYSTEM 17 th JUNE .ibynp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CffqbqRqYGrAGiLI1EvsG1Y2_uuX2vz4
"""

# House Price Prediction System
# Dataset:
#http://hackveda.in/sistec/Housing Modified.csv
# Read the CSV file (house dataset) using pandas
# Import pandas
import pandas as pd
data = pd.read_csv("http://hackveda.in/sistec/Housing_Modified.csv")
data

type(data)

# Dataframe operations
data.head(3)

data.tail(2)

# Extract Knowledge From The Dataset Using KDD Process
# KDD PROCESS has following stages

"""
KDD stages
1.data selection
2.data precprocessing
3.data transformation
4.model interpratation and evaluation
knowledge is extracted
"""

# STAGE1 = DATA SELECTION PROCES
# Show The Rows and Column Data
print("Rows vs Columns in data",data.shape)
print("Rows %d and Columns %d" % data.shape)
# CALCULATE COVARIANCE AMONG FEATURES/VARIABLES
?data.cov()
# CALCULATE CORRELATIONS AMONG FACTORS/FEATURES/VARIABLES

data.cov()

# CALCULATE POSITIVE AND NEGATIVE RELATIONS
# IN PERCENTAGE ( CORELATIONS )
?data.corr()

data.corr()

""" Stage 2= Preprocessing

Case 1: Binary Category
(yes/no,true/false,one/two)
Technique = Label Binarizer
Binarize the variable from text to number
One & Two =1 & 2
"""

""" 
Case 2: N-Category
Technique= Label Encoding
One- Hot encoding / One cold encoding
"""

# Stage 2= Preprocessing 
# Import preprocessing library from sklearn
# Label Binarizer
import sklearn.preprocessing as pp
lb = pp.LabelBinarizer()
data.driveway = lb.fit_transform(data.driveway)
data.recroom= lb.fit_transform(data.recroom)
data.fullbase= lb.fit_transform(data.fullbase)
data.gashw= lb.fit_transform(data.gashw)
data.airco= lb.fit_transform(data.airco)
data.prefarea= lb.fit_transform(data.prefarea)
data.head(2)

data.corr()

# One Hot Encoding ( 2 part of preprocessing)
data_stories = pd.get_dummies(data['stories'],prefix='stories')
data = pd.concat([data,data_stories],axis=1)
del data['stories']
data.head(3)

data.corr()

data.info()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

sns.heatmap(data.corr(),square=True,annot=True,fmt='.1f')

# VIF(VARIANCE INFLEATATION FACTORS)
data.columns

# Create A Seperate List Of Independent_Variables
independent_variables = ['lotsize','bedrooms','bathrms','driveway','recroom','fullbase','gashw','airco','garagepl','prefarea','stories_four',
       'stories_one', 'stories_three', 'stories_two']
independent_variables

# Compute MultiCollinearity
# VIF --> Variance Inflatation Factor
from statsmodels.stats.outliers_influence import variance_inflation_factor
import numpy as np
x = data[independent_variables]
x
y = data['price']
y

# VIF value greater than 10

thresh = 10 #Indicator for VIF

for i in np.arange(0,len(independent_variables)):
  vif=[variance_inflation_factor(x[independent_variables].values,ix)
  for ix in range(x[independent_variables].shape[1])]
  maxloc=vif.index(max(vif))
  if max(vif)>thresh:
    print("VIF :",vif)
    print("dropping :",x[independent_variables].columns[maxloc],"at index",maxloc)
    del independent_variables[maxloc]
   
  else :
    break
    
print('final independent variables:',independent_variables)

independent_variables=['lotsize', 'bathrms', 'driveway', 'recroom', 'fullbase', 'gashw', 'airco', 'garagepl', 'prefarea', 'stories_four', 'stories_one', 'stories_three']
x=data[independent_variables]
y=data['price']
x.head(3)

# split our dataset into training/testing
print(x.shape)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=.80,random_state=1)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

# OLS(Ordinary Least Square)
""""!pip install -U statsmodels
!pip install --upgrade Cython
!pip install --upgrade git+https://github.com/statsmodels/statsmodels
import statsmodel.api as sm"""

import statsmodels.api as sm


lm = sm.OLS(y_train,x_train).fit()
print (lm.summary())

independent_variables=['lotsize', 'bathrms', 'driveway', 'fullbase', 'gashw', 'airco', 'garagepl', 'prefarea', 'stories_four', 'stories_one', 'stories_three']
x=data[independent_variables]
y=data['price']

x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=.80,random_state=1)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

lotsize = int(input('Enter lotsize: '))
bathrms = int(input('Enter Bathrooms: '))
driveway = int(input('Enter Driveway: '))
fullbase = int(input('Enter Fullbase: '))
gashw = int(input('Enter Gas hot water: '))
airco = int(input('Enter Aircondition: '))
garagepl = int(input('Enter Garage place: '))
prefarea = int(input('Enter Prefarea: '))
stories_four = int(input('Enter Story Four: '))
stories_one = int(input('Enter Story one: '))
stories_three = int(input('Enter Story three: '))
# convert these inputs into dictonaries
dict1 = {'lotsize':lotsize,'bathrms':bathrms,'driveway':driveway,'fullbase':fullbase,'gashw':gashw,'airco':airco,'garagepl':garagepl,'prefarea':prefarea,'stories_four':stories_four,'stories_one':stories_one,'stories_three':stories_three}
# convert dictonary into dataframe
data_input = pd.DataFrame(dict1,index=[0],columns=['lotsize','bathrms','driveway','fullbase','gashw','airco','garagepl','prefarea','stories_four','stories_one','stories_three'])
data_input

lm = sm.OLS(y_train,x_train).fit()
print (lm.summary())
predict_price = lm.predict(data_input)
print ('Predicted Price of House is : ',predict_price[0])

# Accuracy
y_pred_price = lm.predict(x_train)
y_pred_price

from sklearn.metrics import r2_score
print ("Model Accuracy : ",r2_score(y_train,y_pred_price))